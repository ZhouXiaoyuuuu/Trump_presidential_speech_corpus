{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac6c8d1-fabd-46d1-8aff-4a5ac5c3ea87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826b633a-9e21-4487-a30f-795aa84af843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load spacy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# We define a class, and name it PresidentialSpeechScraper.\n",
    "class PresidentialSpeechScraper:\n",
    "    # Initialize the class; then define the basic URL, create the president_id, president_name, and speech_data_list in order to transfer or receive data later.\n",
    "    def __init__(self, president_id, president_name):\n",
    "        self.base_url = \"https://millercenter.org/the-presidency/presidential-speeches\"\n",
    "        self.president_id = president_id\n",
    "        self.president_name = president_name\n",
    "        self.speech_data_list = []\n",
    "\n",
    "    # Now we begin to scrape links of speeches.\n",
    "    def scrape_speech_links(self):\n",
    "        # Create a complete link for each specific president, using '?' as a query string to combain self.base_url with target self.president_id. Besides, '=' here represents only the relationship between the key and the value, not the assignment operation, which means the string of this key-value pair is used to tell the server about a particular parameter and its value.\n",
    "        url = f\"{self.base_url}?field_president_target_id[{self.president_id}]={self.president_id}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('div', class_='views-field-title')\n",
    "\n",
    "        for link in links:\n",
    "            title = link.find('a').text\n",
    "            speech_link = link.find('a')['href']\n",
    "            self.speech_data_list.append({'Title': title, 'Link': speech_link})\n",
    "    \n",
    "    # Then we start to scrape more details in every speeches.\n",
    "    def scrape_speech_details(self):\n",
    "        for speech_data in self.speech_data_list:\n",
    "            link = speech_data['Link']\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            title = soup.find('h1').text.strip()\n",
    "            president = soup.find('p', class_='president-name').text.strip()\n",
    "            date = soup.find('p', class_='episode-date').text.strip()\n",
    "            summary = soup.find('div', class_='about-sidebar--intro').p.text.strip()\n",
    "            speech_elements = soup.find_all('div', class_='transcript-inner')\n",
    "            # Connect all the texts in <div> and delete the possible 'Transcript' in texts.\n",
    "            speech = '\\n'.join([element.text.strip().replace(\"Transcript\\n\", \"\") for element in speech_elements])\n",
    "            \n",
    "            # Now we can add the information extracted from the speech details to the dictionary separately.\n",
    "            speech_data.update({\n",
    "                'Title': title,\n",
    "                'President': president,\n",
    "                'Date': date,\n",
    "                'Summary': summary,\n",
    "                'Speech': speech\n",
    "            })\n",
    "            \n",
    "    # Before creating metadata, we need to clean the text.\n",
    "    def clean_text(self, text):\n",
    "        cleaned_text = ' '.join(text.split())\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        return cleaned_text\n",
    "    \n",
    "    # We use spacy to tokenize, lemmatize, and parts-of-speech tag.\n",
    "    def preprocess_text(self, text):\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        return tokens, lemmas, pos_tags\n",
    "        \n",
    "    # Save the data above to csv. files.\n",
    "    def save_to_csv(self):\n",
    "        filename= f\"{self.president_name}_presidential_speeches.csv\"\n",
    "        df = pd.DataFrame(self.speech_data_list)\n",
    "        \n",
    "        df['Tokens'], df['Lemmas'], df['Parts-of-speech'] = zip(*df['Speech'].apply(self.preprocess_text))\n",
    "        df['Filename'] = [f\"{self.president_name}_speech_{i+1}.txt\" for i in range(len(df))]\n",
    "        df = df[['Filename', 'Title', 'Speech', 'Tokens', 'Lemmas', 'Parts-of-speech', 'President', 'Date', 'Summary', 'Link']]\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Speech data saved to {filename}\")\n",
    "    \n",
    "    # Moreover, export each of Trump's speeches as a separate txt document.\n",
    "    def export_individual_speeches_to_txt(self):\n",
    "        for i, speech_data in enumerate(self.speech_data_list):\n",
    "            title = speech_data['Title']\n",
    "            speech_text = speech_data['Speech']\n",
    "            speech_filename = f\"{self.president_name}_speech_{i+1}_{title}.txt\"\n",
    "            with open(speech_filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4957598c-e761-4507-b57c-0203f1fa7182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech data saved to Trump_presidential_speeches.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Trump for the class of PresidentialSpeechScraper.\n",
    "Trump_scraper = PresidentialSpeechScraper(president_id=8396, president_name ='Trump')\n",
    "\n",
    "Trump_scraper.scrape_speech_links()\n",
    "Trump_scraper.scrape_speech_details()\n",
    "\n",
    "Trump_scraper.save_to_csv()\n",
    "\n",
    "Trump_scraper.export_individual_speeches_to_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d61bde-f619-4c71-9ed2-df8d4faea678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
